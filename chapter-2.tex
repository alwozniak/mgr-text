\chapter{Przygotowanie do indeksowania: analiza tekstu}

Lucene przechowuje dane w strukturze zwanej indeksem odwróconym. Zanim jednak indeksowany tekst trafi do tej struktury, musi przejść przez proces analizy. Indeks odwrócony budowany jest na bazie tzw. \emph{tokenów}, które możemy dla uproszczenia zdefiniować jako podstawowe formy słów poddawane indeksowaniu. Proces indeksowania polega, w ogólności, na utworzeniu strumienia tokenów na podstawie zadanego tekstu. Może on obejmować takie operacje jak odfiltrowanie stop-words, sprowadzenie słowa do jego formy podstawowej (poprzed stemming lub przy użyciu analizy morfologicznej), czy wykrycie i zaindekowanie synonimów. 

Analiza tekstu jest problemem nietrywialnym. Od narzędzi ją wykonujących wymagamy, aby różnorodne formy tego samego słowa (jak liczba mnoga, odmiany fleksyjne) były sprowadzone do tej samej bazy, aby poprawnie rozróżniały nazwy własne, numery, daty czy nazwiska. Oczywiście, istniejące obecnie rozwiązania nie spełniają tych wymagań w stu procentach.

Analiza tekstu w Lucene jest zazwyczaj wykonywana w dwóch sytuacjach: podczas indeksowania nowego dokumentu oraz podczas parsowania zapytań. Dodatkowo, tekst pobrany z indeksu może być niekiedy analizowany powtórnie (np. jeśli mają być w nim zaznaczone wystąpienia wyszukiwanych słów).

W Lucene za analizę danych jest odpowiedzialna abstrakcyjna klasa \texttt{Analyzer} oraz implementacje, wśród których najpopularniejszym jest \texttt{StandardAnalyzer}. Istnieje także możliwość utworzenia własnego narzędzia analizy tekstu. 

\section{Struktura Analyzera}

\texttt{Analyzer} jest abstrakcyjną klasą definiującą zarys algorytmu analizy tekstu. Lucene dostarcza kilka jej uniwersalnych implementacji, które będą odpowiednie dla większości zastosowań. Najczęściej wykorzystywaną implementacją jest \texttt{StandardAnalyzer}.

Algorytm analizy tekstu składa się z trzech faz:
\begin{enumerate}
 \item pre-tokenizacji: faza ta na wejściu oczekuje czystego tekstu, jej wyjściem jest również tekst. Polega na odfiltrowaniu niepożądanych znaków lub fragmentów tekstu (takich jak np. znaki interpunkcyjne czy znaczniki HTML). Implementowana jest przez jedną bądź wiele klas typu CharFilter (filtry mogą być łączone w łańcuchy). Faza pre-tokenizacji jest opcjonalna -- wiele analizerów całkowicie ją pomija.
 \item tokenizacji: wejściem dla algorytmu tokenizacji jest czysty tekst, wyjściem -- strumień obiektów typu \texttt{Token}. Tokenizacja (podział tekstu na tokeny) zazwyczaj odbywa się na podstawie białych znaków występujących w tekście. Klasą odpowiedzialną za przeprowadzenie tej fazy jest \texttt{Tokenizer}.
 \item post-tokenizacji: zarówno wejściem jak i wyjściem tej fazy jest strumień \texttt{Tokenów}. W niej odbywają się takie operacje jak stemming, usunięcie stop-words czy wprowadzenie synonimów do strumienia. Post-tokenizacja odbywa się przy użyciu  jednej lub kilku klas implementujących abstrakcyjny \texttt{TokenFilter}. Podobnie, jak w przypadku pre-tokenizacji, filtry można łączyć w łańcuchy.
\end{enumerate}

Tworzenie własnego \texttt{Analyzera} polega na zaimplementowaniu (lub wykorzystaniu dostępnych) komponentów: (opcjonalnie) \texttt{CharFilterów}, \texttt{Tokenizera} oraz \texttt{TokenFilterów}. 

\section{Ekstrakcja treści ze złożonych dokumentów}

Jedyną formą danych akceptowaną przez Lucene jest czysty tekst. Lucene nie dostarcza żadnych narzędzi pozwalających na wyekstrahowanie tekstu z dokumentów o rozbudowanej strukturze (np. w formatach HTML, PDF, MS Word). To po stronie aplikacji używającej biblioteki leży odpowiedzialność parsowania złożonych dokumentów. 